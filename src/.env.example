# example: /home/user/.config/google-chrome/'Profile 1'
CHROME_PROFILE = "<Path to google chrome user profile>"
LINKEDIN_USER = "<User>"
LINKEDIN_PASSWORD = "<Secret>"
MY_SKILLS = ["Add","a","List","of","Your","Skills","Here"]
# Job search queries
QUERIES = ["Query1","Query2","Query3"]
# Maximum number of jobs searched per each query
MAX_NUMBER_OF_JOBS = 100
HEADLESS = True
# Page load timeout is seconds (if <= 0 then timeout is infinite)
LOAD_TIMEOUT = 50
SCRAP_STATE_FILE = "scrap_state.json"
# Database name to save the data (Obviously)
DB_NAME = "jobs.sqlite"
# The following are folders contain system generated data
LOG_FOLDER = "log"
BACKUP_FOLDER = "backup"
OUTPUT_FOLDER = "results"
SCREENSHOT_FOLDER = "screenshots"
# Maximum time the scrapper can restart as a result of a webdriver error
MAX_SCRAPPER_PERSISTENCE = 10

# *** Disconnect Retry Settings ***
# seconds
DISCONNECT_TIMEOUT = 10 
# X times
DISCONNECT_MAX_RETRIES = 5
# Next timeout = previous timeout + previous timeout * multiplier
DISCONNECT_MULTIPLIER = 0.5

# *** Prcess Monitor (procmon) Settings ***
# The CPU usage percentage threshold that is considered high. float: [0-1]
# If =< 0 then it stops monitoring high cpu usage
HIGH_CPU_THRESHOLD = 0
# Each time the cpu usage goes past threshold, procmon adds one count. If 
# continuous counts > MAX_HIGH_CPU_COUNT then the process will restart.
MAX_HIGH_CPU_COUNT = 6
# Maximum times that procmon attempts to restart the scrapper after high cpu usage
MAX_SCRAPPER_RESTART_ATTEMPTS = 6